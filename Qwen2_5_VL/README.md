# VisionZip for Qwen2.5VL

**VisionZip** was originally proposed based on our analysis of visual redundancy in the LLaVA series. Recently, the community has shown increasing interest in applying VisionZip to **Qwen2.5VL**, along with requests for corresponding training code. In response, we implemented our method for Qwen2.5VL.

However, we observed that **Qwen2.5VL already uses PatchMerger** for visual token compression. As a result, the performance gain from VisionZip is less striking compared to LLaVA. We will continue to explore improvements and welcome contributions or suggestions from the community.

---
## Training

We follow the training pipeline of [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory). To use VisionZip in training, simply modify:

```python
from transformers import Qwen2_5_VLForConditionalGeneration
```

to

```python
from qwen2_5vl_visionzip import Qwen2_5_VLForConditionalGeneration
```

To control the number of dominant and contextual visual tokens, modify the code at [line 1916](https://github.com/dvlab-research/VisionZip/blob/68993a31d4f704498a0cc1f0f026754467be65e2/Qwen2_5_VL/qwen2_5vl_visionzip.py#L1916):

```python
dominant_num = int(0.45 * attn_logits.size(0))
contextual_num = max(int(0.05 * attn_logits.size(0)), 1)
```

⚠️ Due to the variable length of visual tokens generated by Qwen2.5VL’s **NaViT visual encoder**, VisionZip currently **only supports `batch_size=1`**.
We support both **FlashAttention-2** and **Eager Attention**.

---

## Evaluation

We follow the evaluation scripts of [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval). Again, simply change:

```python
from transformers import Qwen2_5_VLForConditionalGeneration
```

to

```python
from qwen2_5vl_visionzip import Qwen2_5_VLForConditionalGeneration
```

Make the same adjustment to the dominant/contextual token selection logic at line [line 1916](https://github.com/dvlab-research/VisionZip/blob/68993a31d4f704498a0cc1f0f026754467be65e2/Qwen2_5_VL/qwen2_5vl_visionzip.py#L1916):
```python
dominant_num = int(0.45 * attn_logits.size(0))
contextual_num = max(int(0.05 * attn_logits.size(0)), 1)
```


If you encounter a `value type` error in `apply_rotary_pos_emb_flashatt`, replace the function with:

```python
def apply_rotary_pos_emb_flashatt(
    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor
) -> Tuple[torch.Tensor, torch.Tensor]:
    cos = cos.chunk(2, dim=-1)[0].contiguous()
    sin = sin.chunk(2, dim=-1)[0].contiguous()
    q_embed = apply_rotary_emb(q.float(), cos, sin).type_as(q)
    k_embed = apply_rotary_emb(k.float(), cos, sin).type_as(k)
    return q_embed, k_embed
```

⚠️ Again, due to variable-length visual tokens, evaluation also currently **only supports `batch_size=1`**.
We support both **FlashAttention-2** and **Eager Attention**.

---

## Results
We report some results below. "70%" indicates retaining 70% of visual tokens per sample, with 65% as dominant tokens and 5% as contextual tokens.
"50%" indicates retaining 50% of visual tokens per sample, with 45% as dominant tokens and 5% as contextual tokens.





| Retain Ratio | mme  | mmvet | ocrbench | pope | realworld | docvqa | MathVerse |
| ------------ | ---- | ----- | -------- | ---- | --------- | ------ | --------- |
| 100%         | 2316 | 61.6  | 81.5     | 86.7 | 68.6      | 95.1   | 46.3      |
| 70%          | 2334 | 60.0  | 80.9     | 86.4 | 68.2      | 94.5   | 45.8      |
| 50%          | 2209 | 57.0  | 70.5     | 86.3 | 68.6      | 93.8   | 45.1      |


